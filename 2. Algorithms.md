# Algorithms
___
## Table of Contents

[Introduction](#introduction)  
[Types of Computational Problems](#types)   
[Complexity](#complexity)  
[Precedence and Associativity](#precedence)  

[Algorithmic Techniques](#algotech)   
* [Generate and Test](#gentest)     
* [Exhaustive Search](#exhaustivesearch)  
    * [Exhaustive Search Techniques](#tech)
        * [Compute Solutions](#computesolutions)
        * [Similtaneous and Succesive Search](#simultaneousandsuccesive)
        * [Sorted Candidates](#sortedcandidates)    
        * [Constraint satisfaction](#constraintsatisfaction)    
        * [Equivilant Candidates](#equivilantcandidates) 
        * [Reduce Range](#reducerange)  
        * [Compute Candidate](#computecandidate)    
* [Recursion](#recursion)   
* [Sorting](#sorting)   

___
<a name="introduction"> </a>

## Introduction

An algorithm is an abstraction of steps completed by a computer.\
An algorithm is a step-by-step procedure that takes some input data and, on completion, produces some output data.

A data type is a collection of values and operations on those values.

Algorithms can be implemented by functions, Inputs for different algorithms can be different data types, so we can use the below template:

TEMPLATE ... for Implementing Algorithms using Functions:

**Function:** the name of the function \
**Inputs:** the name and data type of each input \
**Preconditions:** any conditions on the inputs which are true before the input is applied \
**Output:** the name and data type of the result \
**Postconditions:** how the output relates to the inputs 

The data types must be stated: we can’t assume they’re always real numbers.

EXAMPLE:

**Function:** brick volume \
**Inputs:** length, an integer; width, an integer; height, an integer \
**Preconditions:**
length>0;width>0;height>0
length, width and height are in millimetres \
**Output:** volume, an integer \
**Postconditions:**
volume = length × width × height • volume is in cubic millimetres


The algorithm is correctly implemented if the inputs satisfy the preconditions and the outputs satify the post conditions.


An algorithm is in-place if it doesn’t use any additional memory, other than the call stack and a fixed number of local variables for individual items in the sequence, for indices, Booleans, etc.


<a name="types"></a>
## Types of Computational Problems

**Search Problem** - A search problem asks you to find a solution that satisfies a given set of conditions. It's like looking for a specific item in a haystack.

**Decision Problem** - A decision problem asks you to determine whether a solution exists for a given set of conditions. It's like asking a yes-or-no question.

**Optimization Problem** - Find the best solution among all possible solutions.

<a name="complexity"></a>
## Complexity

The complexity of an algorithm is often measured by either
- runtime (time complexity)
- memory (space complexity)

The complexity of an algorithm is the growth rate of its run-times or memory usage as inputs get larger, when executed on the same computational environment (hardware, operating system, programming language and interpreter). 
The complexity is not about how fast the algorithm runs.
Ideally you want to work this out before you get to code the alogorithm to save time refactoring.
Algorithms can take, for example:
- Linear Time ( grows proportionally with the input size )
- Quadratic Time ( grows proportionally to the square of the input size )
- Exponential Time ( The running time grows exponentially with the input size. This is extremely inefficient )
- Logarithmic Time (The running time increases logarithmically with the input size )
- Cubic time ( The running time increases proportionally to the cube of the input size )
- Constant time ( The running time remains constant regardless of the input size. An algorithm that executes a fixed number of operations each with constant complexity, has constant complextity)
- Sublinear (running time grows slower than linear but faster than logarithmic )
- Superlinear - ( The running time grows faster than linear but slower than polynomial )

There are 3 types of notation we can represent complexity of a function with (both time and space) called **Asymptonic Notation**:
- Big O Notation
- Big Omega Notation
- Big Theta Notation

Big O notation is more about the worst case, so the idea that one function is an upper bound of another function. \
Big omega is more about the best case, so the idea that one function is the lower bound of another function. \
Big theta is more about the typical case, so the idea that two functions grow at the same rate.

They indicate that an algorithm’s run-time is proportional to ...something... as the input approached infinity.
It's concerned with that happens to f(n) as n gets larger.

The | symbol is often used in computing to denote the length or magnitude of something. For example, |x| means the absolute value of x, while |A| might represent the number of elements in a set A.

An algorithm with logarithmic complexity Θ(log n) is very efficient: its run-time grows very slowly with input size. If the input doubles, the run-time increases by a fixed amount. We assume that exponentiation takes logarithmic time. An algorithm with log-linear complexity Θ(n log n) is less efficient than a linear algorithm but
much more efficient than a quadratic algorithm.

#### Big Oh notation

Big Oh looks at the upper bound.

#### Big Theta notation

The Θ(1) notation informally means ‘proportional to 1’, which is a roundabout way of saying ‘constant’ because a value that is proportional to a constant (1 in this case) is also constant. While constant complexity could also be written as Θ(2), Θ(57) or with any other fixed value, the convention is to write Θ(1).

___

<a name="associativity"></a>
## Precedence and Associativity

Precedence is about what gets evaluated first.
Associativity is what gets evaluated first when the operations are the same.

| Concept                    | Definition                                                                                         |
|----------------------------|----------------------------------------------------------------------------------------------------|
| **Precedence**              | The order in which operations are evaluated in an expression based on their priority or importance. |
| **Order of Evaluation**     | The sequence in which operations are applied in an expression, determined by precedence.           |
| **Associativity**           | The order in which operations with the same precedence are evaluated.                              |
| **Left-Associative**        | Operations with the same precedence are evaluated from left to right.                              |
| **Right-Associative**       | Operations with the same precedence are evaluated from right to left (e.g., exponentiation).       |



EXAMPLE:
Most arithmetic operators (like +, -, *, /, etc.) in Python are left-associative.
This means that when multiple operators of the same precedence appear in an expression, they are evaluated from left to right.
```python
# Left-associative: subtraction
result = 10 - 5 - 2
print(result)  # Output: 3

```
Exponentiation (**) is an example of an operator in Python that is right-associative. This means that when multiple exponentiation operators appear, they are evaluated from right to left.

```python
# Right-associative: exponentiation
result = 2 ** 3 ** 2
print(result)  # Output: 512
```

___


<a name="types"></a>

## Algorithm Types

1. Procedural
- Algorithms made of instructions assembled in:
    - **Sequence** - instructions one after another
    - **Selection** - alternative sets of instructions
    - **Iteration** - repeats sets of instructions
2. Heuristic
- Heuristic algorithms are problem-solving methods designed to find good, but not necessarily optimal, solutions to complex problems quickly and efficiently. They are often used when exact methods are impractical due to time or computational constraints.

<a name="algotech"></a>

## Algorithmic Techniques



<a name="gentest"></a>

#### Generate and Test

Generate and Test is a general problem-solving paradigm or category of algorithms that follows a two-step process:

Generate: Creates potential solutions (candidates) systematically or randomly
Test: Evaluates each candidate against the problem constraints/requirements.


<a name="exhaustivesearch"></a>

### 1. Exhaustive Search / Brute-Force

Exhaustive search encompases several algortithmic technuques in which each candidate is evaluated against the criteria.

Refers to any algorithm that systematically enumerates all possible candidates for the solution and checks whether each candidate satisfies the problem statement.

Exhausitve search algorithms are examples of generate and test algorithms.
Exhaustive search is usually a slow technique, as it generates many candidates that turn out not to be solutions.
To make an exhaustive search faster, we can generate each candidate as fast as possible, test each candidate as fast as possible, or enumerate as few candidates as possible, i.e. reduce the search
space.

An exhaustive search algorithm with n nested loops can generate all permutations of n items and test which of them satisfy the search conditions. That’s fine if n is small and known in advance, but if n is an input of the problem, how can we generate the permutations.

There are several Exhaustive Search Techniques, some of which reduce the search space.

<a name="tech"></a>

#### Exhaustive Search Techniques


<a name="computesolutions"></a>

##### Compute Solutions 

Sometimes, once you find a solution, you can directly compute other solutions from it and
remove them from the candidates. This reduces the remaining number of candidates to generate
and test. 


<a name="simultaneousandsuccesive"></a>

##### Simultaneous and Successive searches

Simultaneous linear searches allow us to check if the input collection, rather than an individual
item, satisfies the conditions.

You can perform 2 Simultaneous linear searches on the same collection in parrallel.
You can perform searches where the result is dependent on eachother and therefore Successive.

This succssive approach could also be called filtering.

The order of the filters doesn’t matter to obtain a correct result but we should apply them so that they prune the search space quickly.

For example, a store is likely to have many more cheap products and white products than t-shirts, so searching first for t-shirts will lead to a small collection to search.

<a name="sortedcandidates"></a>

##### Sorted Candidates

The search goes through all candidates: it can’t stop early, as there might be more solutions ahead. However, if the candidates are comparable, we can sort them to know when no further solutions are possible. 
For example, if the products are sorted by ascending price, then as
soon as the current candidate costs more than £20.

We can combine sorting and successive filtering.

Best and worst cases rarely happen with real data, but sorting can nevertheless reduce the average run-time.

If the same optimisation criterion is used over and over again, like finding the cheapest black shoes, the cheapest blue dresses, etc., then it’s worth sorting all products by that criterion
before any searches. 

<a name="constraintsatisfaction"></a>

##### Constraint satisfaction

Sometimes we must search for each item that satisfies the conditions. Other times we must search for multiple items that together satisfy the conditions. That’s a form of constraint satisfaction problem (CSP).

<a name="equivilantcandidates"></a>

##### Don't generate equivilant candidates

The algorithm is generating all permutations of the same values because the order in which values are added or multiplied doesn’t matter. All those solutions are equivalent; we should generate only one of them.

<a name="reducerange"></a>

##### Reduce the Range

Another technique is to avoid generating candidates that will fail the test. 

<a name="computecandidate"></a>

##### Compute part of the candidate

Last but not least, when a constraint creates a dependency between values, we can directly compute one value from the others instead of searching for it.

___

<a name="recursion"></a>

## Recursion

A technique where a problem is solved by breaking it down into smaller, similar subproblems, and then solving those subproblems using the same approach.

Recursion is a programming technique in which a function calls itself.
It has been proven that every computable problem can be solved with either an iterative algorithm that uses loops, or a recursive algorithm that doesn’t. 

There’s a group of programming languages, called functional languages, where recursion is the main or only means of iterating, instead of using for- and while-loops. Such languages have specialised but important applications in telecommunications, finance and other domains.

A technique where a problem is solved by breaking it down into smaller, similar subproblems, and then solving those subproblems using the same approach.

A recursive definition must satisfy three conditions:
* The cases must cover all possible input values. Otherwise, the output wouldn’t be defined
for at least one input value. Here, the two cases together cover all natural numbers.
* The lowest possible input value must be a base case because it can’t be further decreased.
Here, the lowest input value is zero. You’ll see examples with more than one base case.
* The recurrence relation must decrease the input to eventually reach the base case(s). In
the next chapter you’ll see recursive definitions that decrease by more than one.

The last step has a recursive call: the algorithm calls the same function, but with a smaller
argument; otherwise the algorithm would never stop. 

To sum up, a recursive algorithm usually consists of four steps:
• check and compute the base case
• decrease the input
• recur
• combine.

A recursive algorithm repeatedly decreases n by one each time, until it can’t be further decreased, i.e. until the input is lowest. At that point we reached the base case and the answer is directly computed. Then, the output for lowest is used to obtain the answer for lowest + 1. That output is then used to obtain the output for lowest + 2 and so on, until we get the output for the original input n.

```python
def factorial(n: int) -> int:
"""Return the factorial of n.
Preconditions: n >= 0
Postconditions:
- if n = 0, then the output is 1
- otherwise the output is 1*2*...*(n-1)*n.
"""
if n == 0:
return 1
else:
return factorial(n-1) * n
factorial(5)

```

<a name="sorting"></a>

## Sorting

Sorting is the process of arranging data into a specific order. 


### BogoSort

Bogosort, also known as permutation sort, stupid sort, or slowsort, is a highly inefficient sorting algorithm based on the generate-and-test paradigm. It should'nt be used.
The bogo sort is an exhaustive search algorithm Here's how it works:

1. Generate Random Permutation: Randomly shuffle the elements of the array.

2. Test if Sorted: Check if the array is sorted. If it is, the algorithm stops.

3. Repeat: If the array is not sorted, repeat the process from step 1.

### Insertion Sort

The insertion sort is a decrease and conquer algorithm.

Theres a recursive version of the Insertion sort and an interative version.

The key idea is the next item to be processed is inserted into an already
sorted part of the sequence. After processing all items, the sequence is sorted.

The recursive version can exhaust the call stack when sorting a sequence with thousands of items or more.

1. Start with the first element

2. Iterate through the unsorted portion: Starting from the second element, compare it with the elements in the sorted portion.

3. Insert the current element into the correct position:

4. If the current element is smaller than the element to its left, shift the larger elements one position to the right to make space.

- Repeat this process until the correct position for the current element is found.

5. Repeat for all elements: Continue this process for each element in the unsorted portion until the entire array is sorted.


### Selection Sort

Selection Sort is a simple comparison-based sorting algorithm. It works by repeatedly selecting the smallest (or largest, depending on the order) element from the unsorted portion of the array and swapping it with the first unsorted element. This process builds the sorted portion of the array one element at a time. 

1. Divide the array into two portions:

- Sorted portion: Initially empty, located at the beginning of the array.

- Unsorted portion: The rest of the array.

2. Find the smallest element in the unsorted portion:

- Iterate through the unsorted portion to locate the smallest element.

3. Swap the smallest element with the first unsorted element:

- Move the smallest element to the end of the sorted portion.

4. Repeat the process:

- Expand the sorted portion by one element and reduce the unsorted portion by one element.

- Continue until the entire array is sorted.

### Merge Sort

Merge Sort is a highly efficient, stable, and comparison-based sorting algorithm that uses the divide-and-conquer approach. It works by recursively dividing the array into smaller subarrays, sorting them, and then merging the sorted subarrays to produce the final sorted array. Here's a step-by-step explanation of how it works:

How Merge Sort Works:

1. Divide:
- Split the array into two halves (or nearly equal parts) recursively until each subarray contains only one element. A single-element array is considered sorted.

2. Conquer:
- Sort the subarrays by merging them back together in a sorted order.

3. Merge:
- Combine two sorted subarrays into a single sorted array by repeatedly comparing the smallest elements of each subarray and placing the smaller element into the result.

Merge sort has two advantages over insertion and selection sort: it has better than quadratic complexity for unsorted input sequences and, being a divide-and-conquer algorithm, can be implemented in parallel.


### Quicksort

Quicksort divides the input sequence in two partitions, recursively sorts each partition and then puts them together. Quicksort divides the items by key: those with smaller keys go into one partition.

Quicksort starts by choosing one item as the pivot, then splits the other items into those smaller and those larger than the pivot. After each partition is sorted, they’re put together: first the smaller items, then the pivot and finally the larger items.

Quick sort is a divide-and-conquer sorting algorithm that is highly efficient and widely used. It works by selecting a pivot element, partitioning the array around the pivot, and then recursively sorting the subarrays.


1. Choose a Pivot
Select an element from the array as the pivot (e.g., first, last, middle, or random element).
2. Partition the Array
3. Rearrange the elements so that:
    Elements smaller than the pivot move to the left.
    Elements greater than the pivot move to the right.
4. Recursively Apply Quick Sort
Apply the same process to the left and right subarrays.


There are variants of the quicksort...

#### Three-way Quicksort

Divide and conquer doesn’t have to be in halves. We can partition the unsorted sequence in three, with the items smaller than, equal to and larger than the pivot. 

#### Quickselect

The quickselect algorithm adapts two-way quicksort. It only recursively searches the partition that includes the sought item, discarding the other partition.


### Pigeon Hole Sort

The algorithms presented so far are comparison sorts: they’re based on comparing pairs of items, or their keys. This shows an example of a distribution sort.
The algorithm gets its name from the pigeonholes used in mail sorting.
Pigeonhole sort uses a map of keys to items with that key. This can be done with a lookup table of length k and a key function that returns natural numbers from 0 to k – 1.
In the first phase (step 3 below) the algorithm distributes the items according to their keys. In the second phase (steps 4 and 5) it collects the items from lowest to highest key.

### Timsort

Timsort is an in-place algorithm with linear complexity in the best case and log-linear complexity in the worst case. You’re unlikely to need another algorithm for sorting arrays in memory.
The Python sort method uses the Timsort algorithm. Timsort was invented by Tim Peters, who wrote the Zen of Python.

Timsort is an efficient hybrid sorting algorithm derived from merge sort and insertion sort. It was designed to perform well on many kinds of real-world data, and it is the standard sorting algorithm for Python and Java’s standard library.

Here’s how Timsort works:

1. Split the Array: The array is divided into smaller sections called runs. Timsort attempts to find natural runs in the array, which are sequences that are already ordered. This helps optimize the sorting process.

2. Sort Runs with Insertion Sort: If a run is too short, Timsort uses insertion sort to sort it. Insertion sort is efficient for small arrays.

3. Merge Runs with Merge Sort: Once all the runs are sorted, Timsort uses a merge sort to combine the runs into a single sorted array.

